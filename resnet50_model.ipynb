{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# input set up\n",
    "taxo_name = \"FFTT\" # or \"Baseline\", \"HFTT\", FFTT\"\n",
    "taxo_idx = 3 # \"Baseline\": [0,1], \"HFTT\": [0,1,2,3], \"FFTT\": [0,1,2,3,4,5]\n",
    "dataset_train = \"train\" \n",
    "dataset_test = \"test\"\n",
    "dataset_extra = \"scitsrcomp\"\n",
    "img_path_train = \"td4cltabs/train\"\n",
    "img_path_test = \"td4cltabs/test\"\n",
    "img_path_extra = \"td4cltabs/SciTSRComp\"\n",
    "\n",
    "Baseline_mappings = [\"Baseline_I\", \"Baseline_II\"]\n",
    "HFTT_mappings = [\"HFTT_Novel_I\", \"HFTT_Novel_II\", \"HFTT_Novel_III\", \"HFTT_Novel_IV\"]\n",
    "FFTT_mappings = [\"FFTT_Novel_I\", \"FFTT_Novel_II\", \"FFTT_Novel_III\",\n",
    "                \"FFTT_Novel_IV\", \"FFTT_Novel_V\", \"FFTT_Novel_VI\"]\n",
    "\n",
    "if taxo_name == \"Baseline\":\n",
    "    mappings = Baseline_mappings\n",
    "    assert taxo_idx < 2\n",
    "elif taxo_name == \"HFTT\":\n",
    "    mappings = HFTT_mappings\n",
    "    assert taxo_idx < 4\n",
    "else:\n",
    "    mappings = FFTT_mappings\n",
    "    assert taxo_idx < 6\n",
    "\n",
    "# read related files\n",
    "with open(f\"td4cltabs/metadata/labels_metadata.json\", \"r\") as input_file:\n",
    "    taxo_id2names = json.load(input_file)[mappings[taxo_idx]]\n",
    "    if taxo_name != \"FFTT\":\n",
    "        taxo_id2names = {int(k): v for k, v in taxo_id2names.items()}\n",
    "    \n",
    "train_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_train}.csv\",\n",
    "                        index_col=[0])\n",
    "test_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_test}.csv\",\n",
    "                        index_col=[0])\n",
    "scitsr_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_extra}.csv\",\n",
    "                        index_col=[0])\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"TD4DLTabs train No. of instances: {}\".format(len(train_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = train_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value/len(train_df)))\n",
    "print(\"---------------------\")\n",
    "print(\"TD4DLTabs test No. of instances: {}\".format(len(test_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = test_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value))\n",
    "print(\"---------------------\")\n",
    "print(\"Scitsrcomp No. of instances: {}\".format(len(scitsr_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = scitsr_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import fastai\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "kfold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if taxo_name != \"FFTT\":\n",
    "    f1_weighted = F1Score(average='weighted')\n",
    "    f1_weighted.name = 'F1(weighted)'\n",
    "    precision_weighted = Precision(average=\"weighted\")\n",
    "    recall_weighted = Recall(average=\"weighted\")\n",
    "\n",
    "else: \n",
    "    f1_macro = F1ScoreMulti(thresh=0.5, average='macro')\n",
    "    f1_macro.name = 'F1(macro)'\n",
    "    f1_weighted = F1ScoreMulti(thresh=0.5, average='weighted')\n",
    "    f1_weighted.name = 'F1(weighted)'\n",
    "    hamming_loss = HammingLossMulti(thresh=0.5)\n",
    "    hamming_loss.name = 'HammingLoss'\n",
    "\n",
    "test_df = pd.concat([test_df]*2)\n",
    "\n",
    "val_pct = {}\n",
    "test_pct = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=1)\n",
    "col = mappings[taxo_idx]\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_df.index, train_df[col])):\n",
    "    print(\"Fold: \", fold)\n",
    "\n",
    "    if taxo_name != \"FFTT\":\n",
    "        train_taxos = DataBlock(\n",
    "            blocks=(ImageBlock, CategoryBlock),\n",
    "            get_x=ColReader('id', pref=img_path_train + os.path.sep),\n",
    "            get_y=ColReader(col),\n",
    "            splitter=IndexSplitter(val_index),\n",
    "        )\n",
    "        train_taxos = train_taxos.new(\n",
    "            item_tfms=Resize((500, 900), method='squish', pad_mode='zeros'),\n",
    "            batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    "        )\n",
    "        \n",
    "        test_taxos = DataBlock(\n",
    "            blocks=(ImageBlock, CategoryBlock),\n",
    "            get_x=ColReader('id', pref=img_path_test + os.path.sep),\n",
    "            get_y=ColReader(col),\n",
    "            splitter=EndSplitter(valid_pct=0.5, valid_last=False),\n",
    "        )\n",
    "        test_taxos = test_taxos.new(\n",
    "            item_tfms=Resize((500, 900), method='squish', pad_mode='zeros'),\n",
    "            batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        train_taxos = DataBlock(\n",
    "            blocks=(ImageBlock, MultiCategoryBlock),\n",
    "            get_x=ColReader('id', pref=img_path_train + os.path.sep),\n",
    "            get_y=ColReader(col, label_delim=' '),\n",
    "            splitter=IndexSplitter(val_index),\n",
    "        )\n",
    "        train_taxos = train_taxos.new(\n",
    "            item_tfms=Resize((500, 900), method='squish', pad_mode='zeros'),\n",
    "            batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    "        )\n",
    "        \n",
    "        test_taxos = DataBlock(\n",
    "            blocks=(ImageBlock, MultiCategoryBlock),\n",
    "            get_x=ColReader('id', pref=img_path_test + os.path.sep),\n",
    "            get_y=ColReader(col, label_delim=' '),\n",
    "            splitter=EndSplitter(valid_pct=0.5, valid_last=False),\n",
    "        )\n",
    "        test_taxos = test_taxos.new(\n",
    "            item_tfms=Resize((500, 900), method='squish', pad_mode='zeros'),\n",
    "            batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n",
    "        )\n",
    "\n",
    "    dls = train_taxos.dataloaders(train_df, bs=16)\n",
    "    test_dls = test_taxos.dataloaders(test_df, bs=16)\n",
    "    test_dls.train = test_dls.valid\n",
    "\n",
    "    # Create a learner\n",
    "    if taxo_name != \"FFTT\":\n",
    "        learn = vision_learner(dls, models.resnet50, \n",
    "                            loss_func=FocalLossFlat(), \n",
    "                            metrics=[error_rate, precision_weighted, recall_weighted, f1_weighted]).to_fp16()\n",
    "    else:\n",
    "        learn = vision_learner(dls, models.resnet50, \n",
    "                            loss_func=BCEWithLogitsLossFlat(),\n",
    "                            metrics=[partial(accuracy_multi, thresh=0.5), f1_weighted, f1_macro, hamming_loss]).to_fp16()\n",
    "\n",
    "    learn.fine_tune(epochs=15, freeze_epochs=5, cbs=[SaveModelCallback(with_opt=True, fname=f\"{col}_{fold}_bestmodel\"),\n",
    "                    EarlyStoppingCallback(monitor='valid_loss', patience=5)])\n",
    "\n",
    "    learn.recorder.plot_loss(skip_start=0, with_valid=True)\n",
    "    plt.figure()\n",
    "    plt.savefig(f'loss_plot({col}_{fold}).png')\n",
    "    \n",
    "    if taxo_name != \"FFTT\":\n",
    "        learn2 = vision_learner(dls, models.resnet50, \n",
    "                            loss_func=FocalLossFlat(), \n",
    "                            metrics=[error_rate, precision_weighted, recall_weighted, f1_weighted], path=\"/kaggle/working/\").to_fp16()\n",
    "    else:\n",
    "        learn2 = vision_learner(dls, models.resnet50, \n",
    "                            loss_func=BCEWithLogitsLossFlat(),\n",
    "                            metrics=[partial(accuracy_multi, thresh=0.5), f1_weighted, f1_macro, hamming_loss]).to_fp16()\n",
    "\n",
    "    learn2 = learn2.load(f\"{col}_{fold}_bestmodel\")\n",
    "\n",
    "    val = learn2.validate()\n",
    "\n",
    "    learn2.dls.valid = test_dls.valid\n",
    "\n",
    "    test = learn2.validate()\n",
    "\n",
    "    if col in val_pct:\n",
    "        val_pct[col].append(val)\n",
    "    else:\n",
    "        val_pct[col] = []\n",
    "        val_pct[col].append(val)\n",
    "        \n",
    "    if col in test_pct:\n",
    "        test_pct[col].append(test)\n",
    "    else:\n",
    "        test_pct[col] = []\n",
    "        test_pct[col].append(test)\n",
    "\n",
    "    with open(f\"{col}_val_pct.pkl\", \"wb\") as file:\n",
    "        pickle.dump(val_pct, file)\n",
    "    with open(f\"{col}_test_pct.pkl\", \"wb\") as file:\n",
    "        pickle.dump(test_pct, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if taxo_name != \"FFTT\":\n",
    "    metrics = ['valid_loss', 'error_rate', 'precision_score', 'recall_score', 'F1(weighted)']\n",
    "else:\n",
    "    metrics = ['valid_loss', 'accuracy_multi', 'F1(weighted)', 'F1(macro)', 'HammingLoss']\n",
    "    \n",
    "print(f\"-------{col}---------\")\n",
    "val = val_pct[col]\n",
    "test = test_pct[col]\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    tmp = []\n",
    "    for i in range(len(val)):\n",
    "        tmp.append(val[i][idx])\n",
    "\n",
    "    print(f'{metric} Validation: \\tmean: {np.mean(tmp)} \\tstd: {np.std(tmp)}')\n",
    "\n",
    "    tmp = []\n",
    "    for i in range(len(test)):\n",
    "        tmp.append(test[i][idx])\n",
    "    print(f'{metric} Test:       \\tmean: {np.mean(tmp)} \\tstd: {np.std(tmp)}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
