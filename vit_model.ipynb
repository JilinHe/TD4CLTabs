{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# input set up\n",
    "taxo_name = \"FFTT\" # or \"Baseline\", \"HFTT\", FFTT\"\n",
    "taxo_idx = 3 # \"Baseline\": [0,1], \"HFTT\": [0,1,2,3], \"FFTT\": [0,1,2,3,4,5]\n",
    "dataset_train = \"train\" \n",
    "dataset_test = \"test\"\n",
    "dataset_extra = \"scitsrcomp\"\n",
    "img_path_train = \"td4cltabs/train\"\n",
    "img_path_test = \"td4cltabs/test\"\n",
    "img_path_extra = \"td4cltabs/SciTSRComp\"\n",
    "\n",
    "Baseline_mappings = [\"Baseline_I\", \"Baseline_II\"]\n",
    "HFTT_mappings = [\"HFTT_Novel_I\", \"HFTT_Novel_II\", \"HFTT_Novel_III\", \"HFTT_Novel_IV\"]\n",
    "FFTT_mappings = [\"FFTT_Novel_I\", \"FFTT_Novel_II\", \"FFTT_Novel_III\",\n",
    "                \"FFTT_Novel_IV\", \"FFTT_Novel_V\", \"FFTT_Novel_VI\"]\n",
    "\n",
    "if taxo_name == \"Baseline\":\n",
    "    mappings = Baseline_mappings\n",
    "    assert taxo_idx < 2\n",
    "elif taxo_name == \"HFTT\":\n",
    "    mappings = HFTT_mappings\n",
    "    assert taxo_idx < 4\n",
    "else:\n",
    "    mappings = FFTT_mappings\n",
    "    assert taxo_idx < 6\n",
    "\n",
    "# read related files\n",
    "with open(f\"td4cltabs/metadata/labels_metadata.json\", \"r\") as input_file:\n",
    "    taxo_id2names = json.load(input_file)[mappings[taxo_idx]]\n",
    "    if taxo_name != \"FFTT\":\n",
    "        taxo_id2names = {int(k): v for k, v in taxo_id2names.items()}\n",
    "    \n",
    "train_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_train}.csv\",\n",
    "                        index_col=[0])\n",
    "test_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_test}.csv\",\n",
    "                        index_col=[0])\n",
    "scitsr_df = pd.read_csv(f\"td4cltabs/metadata/{mappings[taxo_idx]}/{dataset_extra}.csv\",\n",
    "                        index_col=[0])\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"TD4DLTabs train No. of instances: {}\".format(len(train_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = train_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value/len(train_df)))\n",
    "print(\"---------------------\")\n",
    "print(\"TD4DLTabs test No. of instances: {}\".format(len(test_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = test_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value))\n",
    "print(\"---------------------\")\n",
    "print(\"Scitsrcomp No. of instances: {}\".format(len(scitsr_df[mappings[taxo_idx]].values)))\n",
    "if taxo_name != \"FFTT\":\n",
    "    taxo_freqs = scitsr_df[mappings[taxo_idx]].value_counts().rename(index=taxo_id2names)\n",
    "    for freq_name, freq_value in taxo_freqs.items():\n",
    "        print(\"\\tNo. of {}: {}\".format(freq_name, freq_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# key=\"5c8a29*********************\"\n",
    "# input with your key\n",
    "wandb.login(key=input_with_your_key)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import v2\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.io import read_image\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss, multilabel_confusion_matrix\n",
    "from datasets import load_dataset as ds_load_dataset, DatasetDict as ds_DatasetDict, Dataset as ds_Dataset, ClassLabel as ds_ClassLabel, Features as ds_Features, Image as ds_Image, Sequence as ds_Sequence\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    if taxo_name != \"FFTT\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return dict(accuracy=accuracy_score(labels, predictions),\n",
    "                   precision=precision_score(labels, predictions, average=\"weighted\"),\n",
    "                   recall=recall_score(labels, predictions, average=\"weighted\"),\n",
    "                   f1_score=f1_score(labels, predictions, average=\"weighted\"),\n",
    "                   confusion_matrix=confusion_matrix(labels, predictions).tolist())\n",
    "\n",
    "    else:\n",
    "        predictions = (predictions > 0.5).astype(int)  # Adjust the threshold as needed\n",
    "        hamming_loss_value = hamming_loss(labels, predictions)\n",
    "        multilabel_cm = multilabel_confusion_matrix(labels, predictions)\n",
    "\n",
    "        precision_micro = precision_score(labels, predictions, average='micro')\n",
    "        recall_micro = recall_score(labels, predictions, average='micro')\n",
    "        f1_micro = f1_score(labels, predictions, average='micro')\n",
    "\n",
    "        precision_macro = precision_score(labels, predictions, average='macro')\n",
    "        recall_macro = recall_score(labels, predictions, average='macro')\n",
    "        f1_macro = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "        return dict(\n",
    "            accuracy=accuracy_score(labels, predictions),\n",
    "            precision_micro=precision_micro,\n",
    "            recall_micro=recall_micro,\n",
    "            f1_score_micro=f1_micro,\n",
    "            precision_macro=precision_macro,\n",
    "            recall_macro=recall_macro,\n",
    "            f1_score_macro=f1_macro,\n",
    "            hamming_loss=hamming_loss_value,\n",
    "            confusion_matrix=multilabel_cm.tolist()\n",
    "        )\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    \n",
    "    if taxo_name != \"FFTT\":\n",
    "        labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "    else:\n",
    "        labels = torch.FloatTensor([example[\"multi_label\"] for example in examples])\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"eval_loss\"\n",
    "col = mappings[taxo_idx]\n",
    "num_classes = len(taxo_id2names.keys())\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k', do_rescale = False, return_tensors = 'pt')\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
    "    v2.ToTensor(),\n",
    "    v2.Pad(padding=0, padding_mode='constant'),\n",
    "    normalize\n",
    " ])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
    "    v2.ToTensor(),\n",
    "    v2.Pad(padding=0, padding_mode='constant'),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [train_transform(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def test_transforms(examples):\n",
    "    examples['pixel_values'] = [test_transform(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "kfold = 4\n",
    "\n",
    "val_pct = {}\n",
    "test_pct = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=1)\n",
    "\n",
    "images = [Image.open(f\"{img_path_train}/{id}\") for id in train_df['id'].values]\n",
    "if taxo_name != \"FFTT\":\n",
    "    labels = [str(lb) for lb in train_df[col].values]\n",
    "else:\n",
    "    labels = train_df[col].values\n",
    "\n",
    "test_images = [Image.open(f\"{img_path_test}/{id}\") for id in test_df['id'].values]\n",
    "if taxo_name != \"FFTT\":\n",
    "    test_labels = [str(lb) for lb in test_df[col].values]\n",
    "else:\n",
    "    test_labels = [\n",
    "        [1 if str(i + 1) in lbs.split(' ') else 0 for i in range(num_classes)]\n",
    "        for lbs in test_df[col].values\n",
    "    ]\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(images, labels)):\n",
    "    print(fold)\n",
    "    print(len(train_indices))\n",
    "    \n",
    "    if taxo_name != \"FFTT\":\n",
    "        features = ds_Features(\n",
    "            {\n",
    "                \"image\": ds_Image(decode=True),\n",
    "                \"label\": ds_ClassLabel(names=list(set(labels)))\n",
    "            }\n",
    "        )\n",
    "        train_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": [images[i] for i in train_indices],\n",
    "                \"label\": [labels[i] for i in train_indices]\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "        val_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": [images[i] for i in val_indices],\n",
    "                \"label\": [labels[i] for i in val_indices]\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "        test_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": test_images,\n",
    "                \"label\": test_labels\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "        \n",
    "        idx2label = {idx: label for idx, label in enumerate(labels)}\n",
    "        label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "    else:\n",
    "        train_labels = [\n",
    "            [1 if str(i + 1) in lbs.split(' ') else 0 for i in range(num_classes)]\n",
    "            for lbs in train_df[col].values\n",
    "        ]\n",
    "        features = ds_Features(\n",
    "            {\n",
    "                \"image\": ds_Image(decode=True),\n",
    "                \"multi_label\": ds_Sequence(ds_ClassLabel(names=[i for i in range(1,num_classes+1)]))\n",
    "            }\n",
    "        )\n",
    "        train_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": [images[i] for i in train_indices],\n",
    "                \"multi_label\": [train_labels[i] for i in train_indices]\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "        val_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": [images[i] for i in val_indices],\n",
    "                \"multi_label\": [train_labels[i] for i in val_indices]\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "        test_dataset = ds_Dataset.from_dict(\n",
    "            {\n",
    "                \"image\": test_images,\n",
    "                \"multi_label\": test_labels\n",
    "            },\n",
    "            features=features,\n",
    "        )\n",
    "    \n",
    "    final_dataset = ds_DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "\n",
    "    # Set the transforms\n",
    "    final_dataset['train'].set_transform(train_transforms)\n",
    "    final_dataset['validation'].set_transform(test_transforms)\n",
    "    final_dataset['test'].set_transform(test_transforms)\n",
    "    \n",
    "    if taxo_name != \"FFTT\":\n",
    "        model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                      id2label=idx2label,\n",
    "                                                      label2id=label2idx,\n",
    "                                                      ignore_mismatched_sizes=True)\n",
    "    else:\n",
    "        model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                      num_labels=num_classes,\n",
    "                                                      problem_type=\"multi_label_classification\")\n",
    "        \n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k', \n",
    "                                                  do_rescale=False, \n",
    "                                                  return_tensors='pt')\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"table-classification\",\n",
    "        use_cpu = False,\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=metric_name,\n",
    "        logging_dir='logs',\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=final_dataset['train'],\n",
    "        eval_dataset=final_dataset['validation'],\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    outputs = trainer.predict(final_dataset['test'])\n",
    "    \n",
    "    print(outputs.metrics)\n",
    "    \n",
    "    try:\n",
    "        for k,v in outputs.metrics.items():\n",
    "            if k in val_pct:\n",
    "                val_pct[k].append(v)\n",
    "            else:\n",
    "                val_pct[k] = []\n",
    "                val_pct[k].append(v)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"val_pct.pkl\", \"wb\") as file:\n",
    "    pickle.dump(val_pct, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in val_pct.items():\n",
    "    print(f'{k} Validation: \\tmean: {np.mean(v)} \\tstd: {np.std(v)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
